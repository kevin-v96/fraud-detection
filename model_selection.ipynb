{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d27b7767",
   "metadata": {},
   "source": [
    "We'll explore some unsupervised models and see what works best for our purposes in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870e661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/flagright.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077dd06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destinationCountry</th>\n",
       "      <th>destinationCurrency</th>\n",
       "      <th>destinationAmount</th>\n",
       "      <th>originCountry</th>\n",
       "      <th>originCurrency</th>\n",
       "      <th>originAmount</th>\n",
       "      <th>state</th>\n",
       "      <th>destinationMethod</th>\n",
       "      <th>originMethod</th>\n",
       "      <th>transactionId</th>\n",
       "      <th>originUserId</th>\n",
       "      <th>destinationUserId</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>week_day_sin</th>\n",
       "      <th>week_day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>10132.80</td>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>10132.80</td>\n",
       "      <td>CREATED</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>bd70fcaebc254c23b07b29fd994ba5f2</td>\n",
       "      <td>29529892-22d3-4a74-b6f2-fbe1d5ee8b6f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.903356</td>\n",
       "      <td>-0.428892</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>145653.93</td>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>145653.93</td>\n",
       "      <td>CREATED</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>c9f8913d0bd548838e97bd6a609dbc45</td>\n",
       "      <td>0b85951b-c817-499e-ad17-453e5feaf87c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.903356</td>\n",
       "      <td>-0.428892</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>6311.00</td>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>6311.00</td>\n",
       "      <td>CREATED</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>4de9f33636cf44378f748f723ee4ac87</td>\n",
       "      <td>29529892-22d3-4a74-b6f2-fbe1d5ee8b6f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.903356</td>\n",
       "      <td>-0.428892</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>400000.00</td>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>400000.00</td>\n",
       "      <td>CREATED</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>12b97d4eb51940d0886f609903fb2154</td>\n",
       "      <td>2f6ec341-9075-4aaa-9db2-9fd5d8597f97</td>\n",
       "      <td>0b85951b-c817-499e-ad17-453e5feaf87c</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.903356</td>\n",
       "      <td>-0.428892</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>45.00</td>\n",
       "      <td>IN</td>\n",
       "      <td>INR</td>\n",
       "      <td>45.00</td>\n",
       "      <td>CREATED</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>GENERIC_BANK_ACCOUNT</td>\n",
       "      <td>32c6bc06f67b4e2b8222f76c8268cea9</td>\n",
       "      <td>0b85951b-c817-499e-ad17-453e5feaf87c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.903356</td>\n",
       "      <td>-0.428892</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  destinationCountry destinationCurrency  destinationAmount originCountry  \\\n",
       "0                 IN                 INR           10132.80            IN   \n",
       "1                 IN                 INR          145653.93            IN   \n",
       "2                 IN                 INR            6311.00            IN   \n",
       "3                 IN                 INR          400000.00            IN   \n",
       "4                 IN                 INR              45.00            IN   \n",
       "\n",
       "  originCurrency  originAmount    state     destinationMethod  \\\n",
       "0            INR      10132.80  CREATED  GENERIC_BANK_ACCOUNT   \n",
       "1            INR     145653.93  CREATED  GENERIC_BANK_ACCOUNT   \n",
       "2            INR       6311.00  CREATED  GENERIC_BANK_ACCOUNT   \n",
       "3            INR     400000.00  CREATED  GENERIC_BANK_ACCOUNT   \n",
       "4            INR         45.00  CREATED  GENERIC_BANK_ACCOUNT   \n",
       "\n",
       "           originMethod                     transactionId  \\\n",
       "0  GENERIC_BANK_ACCOUNT  bd70fcaebc254c23b07b29fd994ba5f2   \n",
       "1  GENERIC_BANK_ACCOUNT  c9f8913d0bd548838e97bd6a609dbc45   \n",
       "2  GENERIC_BANK_ACCOUNT  4de9f33636cf44378f748f723ee4ac87   \n",
       "3  GENERIC_BANK_ACCOUNT  12b97d4eb51940d0886f609903fb2154   \n",
       "4  GENERIC_BANK_ACCOUNT  32c6bc06f67b4e2b8222f76c8268cea9   \n",
       "\n",
       "                           originUserId                     destinationUserId  \\\n",
       "0  29529892-22d3-4a74-b6f2-fbe1d5ee8b6f                                   NaN   \n",
       "1  0b85951b-c817-499e-ad17-453e5feaf87c                                   NaN   \n",
       "2  29529892-22d3-4a74-b6f2-fbe1d5ee8b6f                                   NaN   \n",
       "3  2f6ec341-9075-4aaa-9db2-9fd5d8597f97  0b85951b-c817-499e-ad17-453e5feaf87c   \n",
       "4  0b85951b-c817-499e-ad17-453e5feaf87c                                   NaN   \n",
       "\n",
       "   hour_sin  hour_cos   day_sin   day_cos  week_day_sin  week_day_cos  \n",
       "0  0.258819 -0.965926 -0.903356 -0.428892      0.433884     -0.900969  \n",
       "1  0.258819 -0.965926 -0.903356 -0.428892      0.433884     -0.900969  \n",
       "2  0.258819 -0.965926 -0.903356 -0.428892      0.433884     -0.900969  \n",
       "3  0.258819 -0.965926 -0.903356 -0.428892      0.433884     -0.900969  \n",
       "4  0.258819 -0.965926 -0.903356 -0.428892      0.433884     -0.900969  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d58214",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['timestamp', 'amount']\n",
    "categorical_columns = ['transactionId', 'originUserId', 'destinationUserId'] #separate the numerical and categorical columns for easier handling\n",
    "\n",
    "\n",
    "features = pd.DataFrame(index = df.index)\n",
    "features[numerical_columns] = df[numerical_columns]\n",
    "features[categorical_columns] = df[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ade18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Encode Columns using \n",
    "labelencoder_X = LabelEncoder()\n",
    "X_labelEncoder = features[numerical_columns]\n",
    "X_labelEncoder['transactionId'] = labelencoder_X.fit_transform(features['transactionId']) # we can use LabelEncoder or OneHotEncoder for the categorical features\n",
    "X_labelEncoder['originUserId'] = labelencoder_X.fit_transform(features['originUserId'])   #both of them accomplish the same thing, but the sklearn documentation\n",
    "X_labelEncoder['destinationUserId'] = labelencoder_X.fit_transform(features['destinationUserId']) #recommends that users use OneHotEncoder for features and LabelEncoder for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e028f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labelEncoder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(random_state=42).fit(X_labelEncoder)\n",
    "clf.predict(X_labelEncoder[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0137fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.decision_function(X_labelEncoder[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8532c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_labelEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1994a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(preds == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6284781",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(preds == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "print(calinski_harabasz_score(X_labelEncoder, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['amount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47cb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(features['amount'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5cb2279",
   "metadata": {},
   "source": [
    "We can check what the distribution of the transaction amount is so we can get a baseline amount to flag bigger transactions. This can help us compare our model with a rule-based approach where you automaticall flag the biggest transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05273c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['amount'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['isFlagged'] = features['amount'] > 135000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6da86017",
   "metadata": {},
   "source": [
    "The calinski_harabasz_score is a cluster analysis metric that tells you how well your model is making clusters. The score is defined as ratio of the sum of between-cluster dispersion and of within-cluster dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calinski_harabasz_score(X_labelEncoder, features['isFlagged']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e889864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with open('model.joblib', 'wb') as f:\n",
    "    joblib.dump(clf,f)\n",
    "\n",
    "\n",
    "with open('model.joblib', 'rb') as f:\n",
    "    predictor = joblib.load(f)\n",
    "\n",
    "print(\"Testing following input: \")\n",
    "print(X_labelEncoder[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampInput = [[1662358419786, 10132.8, 1598, 3, 22]]\n",
    "print(type(sampInput))\n",
    "print(predictor.predict(sampInput))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c1ba8c8",
   "metadata": {},
   "source": [
    "Below, I tried an autoencoder approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define your autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, encoding_size),\n",
    "            nn.ReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define your training and testing data\n",
    "train_data = features[:1700]\n",
    "test_data = features[1700:]\n",
    "\n",
    "# Preprocess categorical features\n",
    "categorical_columns = ['transactionId','originUserId','destinationUserId','isFlagged']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_data[col].values) + list(test_data[col].values))\n",
    "    train_data[col] = le.transform(train_data[col])\n",
    "    test_data[col] = le.transform(test_data[col])\n",
    "\n",
    "# Normalize your data\n",
    "mean = train_data.mean()\n",
    "std = train_data.std()\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "# Define a custom dataset for your data\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "# Create data loaders\n",
    "train_dataset = TabularDataset(train_data)\n",
    "test_dataset = TabularDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize your autoencoder model and optimizer\n",
    "input_size = len(train_data.columns)\n",
    "encoding_size = 10\n",
    "model = Autoencoder(input_size, encoding_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define your loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the autoencoder\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss/len(train_loader)))\n",
    "\n",
    "# Evaluate the autoencoder\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    anomaly_scores = []\n",
    "    for data in test_loader:\n",
    "        inputs = data\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.sum((outputs - inputs)**2, dim=1)\n",
    "        anomaly_scores += loss.cpu().numpy().tolist()\n",
    "\n",
    "# Detect anomalies using a threshold\n",
    "threshold = 0.1\n",
    "anomaly_labels = [1 if score > threshold else 0 for score in anomaly_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a04bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "481b9bdb",
   "metadata": {},
   "source": [
    "After trying a couple of approaches, I decided on using IsolationForest since it provides what we need (an outlier boolean and a confidence score) out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2b7ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
